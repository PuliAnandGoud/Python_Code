Preprocessing - Introduction
Preprocessing is a step, in which raw data is modified or transformed into a format, suitable for further downstream processing.

scikit-learn provides many preprocessing utilities such as:
	Standardization mean removal
	Scaling
	Normalization
	Binarization
	One Hot Encoding
	Label Encoding
	Imputation
1. Standardization: Standardization or Mean Removal is the process of transforming each feature vector into a normal distribution with mean 0 and variance 1.

# An example with its output is shown in the next two cards, which requires the following imports.
import sklearn.preprocessing as preprocessing
standardizer = preprocessing.StandardScaler()
standardizer = standardizer.fit(breast_cancer.data)
breast_cancer_standardized = standardizer.transform(breast_cancer.data)

print('Mean of each feature after Standardization :\n\n')
print(breast_cancer_standardized.mean(axis=0))
print('\nStd. of each feature after Standardization :\n\n')
print(breast_cancer_standardized.std(axis=0))

2. Scaling: Scaling transforms existing data values to lie between a minimum and maximum value.

	MinMaxScaler transforms data to range 0 and 1.
	MaxAbsScaler transforms data to range -1 and 1.

# MinMaxScaler with specified range
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 10)).fit(breast_cancer.data)
breast_cancer_minmaxscaled10 = min_max_scaler.transform(breast_cancer.data)
# In the above example, data is transformed to range 0 and 10.

Using MaxAbsScaler: the maximum absolute value of each feature is scaled to unit size, i.e., 1. It is intended for data that is previously centered at sparse or zero data.
# Example for MaxAbsScaler
max_abs_scaler = preprocessing.MaxAbsScaler().fit(breast_cancer.data)
breast_cancer_maxabsscaled = max_abs_scaler.transform(breast_cancer.data)
By default, MaxAbsScaler transforms data to the range -1 and 1.

Using MaxAbsScaler: the maximum absolute value of each feature is scaled to unit size, i.e., 1. It is intended for data that is previously centered at sparse or zero data.

#Example for MaxAbsScaler
max_abs_scaler = preprocessing.MaxAbsScaler().fit(breast_cancer.data)
breast_cancer_maxabsscaled = max_abs_scaler.transform(breast_cancer.data)
# By default, MaxAbsScaler transforms data to the range -1 and 1.

3. Normalization
	Normalization scales each sample to have a unit norm.
	Normalization can be achieved with 'l1', 'l2', and 'max' norms.
	'l1' norm makes the sum of absolute values of each row as 1, and 'l2' norm makes the sum of squares of each row as 1.
	'l1' norm is insensitive to outliers.
	By default l2 norm is considered. Hence, removing outliers is recommended before applying l2 norm.

normalizer = preprocessing.Normalizer(norm='l1').fit(breast_cancer.data)
breast_cancer_normalized = normalizer.transform(breast_cancer.data)
In above example, l1 norm is used with norm parameter.

4. Binarization: Binarization is the process of transforming data points to 0 or 1 based on a given threshold.
Any value above the threshold is transformed to 1, and any value below the threshold is transformed to 0.
By default, a threshold of 0 is used.

binarizer = preprocessing.Binarizer(threshold=3.0).fit(breast_cancer.data)
breast_cancer_binarized = binarizer.transform(breast_cancer.data)
print(breast_cancer_binarized[:5,:5])


5. OneHotEncoder: it converts categorical integer values into one-hot vectors. In an on-hot vector, every category is transformed into a binary attribute having only 0 and 1 values.

# An example creating two binary attributes for the categorical integers 1 and 2, is shown in the next slide.
onehotencoder = preprocessing.OneHotEncoder()
onehotencoder = onehotencoder.fit([[1], [1], [1], [2], [2], [1]])

# Transforming category values 1 and 2 to one-hot vectors
print(onehotencoder.transform([[1]]).toarray())
print(onehotencoder.transform([[2]]).toarray())

6. Imputation: Imputation replaces missing values with either median, mean, or the most common value of the column or row in which the missing values exist.

Below example replaces missing values, represented by np.nan, with the mean of respective column (axis 0).
# Example
imputer = preprocessing.Imputer(missing_values='NaN', strategy='mean')
imputer = imputer.fit(breast_cancer.data)
breast_cancer_imputed = imputer.transform(breast_cancer.data)

7. Label Encoding: Label Encoding is a step in which, in which categorical features are represented as categorical integers. An example of transforming categorical values ["benign","malignant"]into[0, 1]` is shown below.

#Example
labels = ['malignant', 'benign', 'malignant', 'benign']
labelencoder = preprocessing.LabelEncoder()
labelencoder = labelencoder.fit(labels)
bc_labelencoded = labelencoder.transform(breast_cancer.target_names)

Using MinMaxScaler
# Example for MinMaxScaler
min_max_scaler = preprocessing.MinMaxScaler().fit(breast_cancer.data)
breast_cancer_minmaxscaled = min_max_scaler.transform(breast_cancer.data)

By default, transformation occurs to a range of 0 and 1. 
It can also be customized with feature_range argument as shown in next example.

----------------------------------------------
Nearest Neighbors: method is used to determine a predefined number of data points that are closer to a sample point and predict its label.

sklearn.neighbors provides utilities for unsupervised and supervised neighbors-based learning methods.

scikit-learn implements two different nearest neighbors classifiers:
1. KNeighborsClassifier
2. RadiusNeighborsClassifier

Nearest Neighbor Classifiers
1. KNeighborsClassifier classifies based on k nearest neighbors of every query point, where k is an integer value specified by the user.
2. RadiusNeighborsClassifier classifies based on the number of neighbors present in a fixed radius r of every training point.

Nearest Neighbors Regression: scikit-learn implements the following two regressors:

1. KNeighborsRegressor predicts based on the k nearest neighbors of each query point.
2. RadiusNeighborsRegressor predicts based on the neighbors present in a fixed radius r of the query point.

# Demo of KNeighborsClassifier
import sklearn.datasets as datasets
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
cancer = datasets.load_breast_cancer()  # Loading the data set

# Building a Model of KNN classifier
X_train, X_test, Y_train, Y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)
knn_classifier = KNeighborsClassifier()   
knn_classifier = knn_classifier.fit(X_train, Y_train)

# Determining Accuracy of the Model
print('Accuracy of Train Data :', knn_classifier.score(X_train,Y_train))
print('Accuracy of Test Data :', knn_classifier.score(X_test,Y_test))


----------------------------------------------
Decision Tree 
sklearn.tree
DecisionTreeClassifier 
DecisionTreeRegressor 
## Classification code
from sklearn.tree import DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier()   # Further the model is improved with change in max_depth value to 2.
dt_classifier = dt_classifier.fit(X_train, Y_train) 

print('Accuracy of Train Data :', dt_classifier.score(X_train,Y_train))
print('Accuracy of Test Data :', dt_classifier.score(X_test,Y_test))

## Regression 
from sklearn.tree import DecisionTreeRegressor
dt_reg = DecisionTreeRegressor() # max_depth = 3
dt_reg = dt_reg.fit(X_train, y_train)
print(dt_reg.score(X_train, y_train))
print(dt_reg.score(X_test, y_test))
----------------------------------------------
Ensemble Methods : Ensemble methods combine predictions of other learning algorithms, to improve the generalization.

Ensemble methods are two types:
1. Averaging Methods: They build several base estimators independently and finally average their predictions.
	E.g.: Bagging Methods, Forests of randomised trees
2. Boosting Methods: They build base estimators sequentially and tree to reduce the bias of the combined estimator.
	E.g.: Adaboost, Gradient Tree Boosting

Bagging Methods: draw random subsets of the original dataset, build an estimator and aggregate individual results to form a final one.
	BaggingClassifier and BaggingRegressor are the utilities from sklearn.ensemble to deal with Bagging.

Randomized Trees: sklearn.ensemble offers two types of algorithms based on randomized trees: Random Forest and Extra randomness algorithms.
	RandomForestClassifier and RandomForestRegressor classes are used to deal with random forests.
	In random forests, each estimator is built from a sample drawn with replacement from the training set.

Randomized Trees: ExtraTreesClassifier and ExtraTreesRegressor classes are used to deal with extremely randomized forests.
	In extremely randomized forests, more randomness is introduced, which further reduces the variance of the model

Boosting Methods: Boosting Methods combine several weak models to create a improvised ensemble.
	sklearn.ensemble also provides the following boosting algorithms:
	i) AdaBoostClassifier
	ii) GradientBoostingClassifier
# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier()
rf_classifier = rf_classifier.fit(X_train, Y_train) 
print('Accuracy of Train Data :', rf_classifier.score(X_train,Y_train))
print('Accuracy of Test Data :', rf_classifier.score(X_test,Y_test))
# Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor
rf_regreesor = RandomForestRegressor()  # max_depth from 3 to 5 and n_estimators to 50, 100, 200 values.
rf_regreesor = rf_regreesor.fit(X_train, y_train)
print(rf_regreesor.score(X_train, y_train))
print(rf_regreesor.score(X_test, y_test))

----------------------------------------------
Support Vector Machines (SVMs): separates data points based on decision planes, which separates objects belonging to different classes in a higher dimensional space.
SVM algorithm uses the best suitable kernel, which is capable of separating data points into two or more classes.
Commonly used kernels are:
	1. linear
	2. polynomial
	3. rbf
	4. sigmoid

Support Vector Classification : scikit-learn provides the following three utilities for performing Support Vector Classification.
	i) SVC,
	ii) NuSVC: Same as SVC but uses a parameter to control the number of support vectors.
	iii) LinearSVC: Similar to SVC with parameter kernel taking linear value.
Support Vector Regression : scikit-learn provides the following three utilities for performing Support Vector Regression.
	i) SVR
	ii) NuSVR
	iii) LinearSVR

Advantages of SVMs
1. SVM can distinguish the classes in a higher dimensional space.
2. SVM algorithms are memory efficient.
3. SVMs are versatile, and a different kernel can be used by a decision function

Disadvantages of SVMs
1. SVMs do not perform well on high dimensional data with many samples.
2. SVMs work better only with Preprocessed data.
3. They are harder to visualize

Demo of Support Vector Classification: An example of creating an SVM classifier is shown below.
The shown model overfits the training data.
# SVM Code
from sklearn.svm import SVC
svm_classifier = SVC() # set C values to 5, 100, 400.
svm_classifier = svm_classifier.fit(X_train, Y_train) 
print('Accuracy of Train Data :', svm_classifier.score(X_train,Y_train))
print('Accuracy of Test Data :', svm_classifier.score(X_test,Y_test))

Improving Accuracy Using Scaled Data In the following example, scaled input data is used to improve the accuracy of SVM classifier.

import sklearn.preprocessing as preprocessing
standardizer = preprocessing.StandardScaler()
standardizer = standardizer.fit(cancer.data)
cancer_standardized = standardizer.transform(cancer.data)

svm_classifier = SVC()
svm_classifier = svm_classifier.fit(X_train, Y_train) 

# Viewing the Classification Report
from sklearn import metrics
Y_pred = svm_classifier.predict(X_test)
print('Classification report : \n',metrics.classification_report(Y_test, Y_pred))

----------------------------------------------
Introduction to Clustering : Clustering is one of the unsupervised learning technique.
The technique is typically used to group data points into clusters based on a specific algorithm.
Major clustering algorithms that can be implemented using scikit-learn are:

	K-means Clustering
	Agglomerative clustering
	DBSCAN clustering
	Mean-shift clustering
	Affinity propagation
	Spectral clustering

1. K-Means Clustering: In K-means Clustering entire data set is grouped into k clusters.

Steps involved are:
	k centroids are chosen randomly.
	The distance of each data point from k centroids is calculated. A data point is assigned to the nearest cluster.
	Centroids of k clusters are recomputed.
	The above steps are iterated till the number of data points a cluster reach convergence.
KMeans from sklearn.cluster can be used for K-means clustering.

2. Agglomerative Hierarchical Clustering: Agglomerative Hierarchical Clustering is a bottom-up approach.

Steps involved are:
	Each data point is treated as a single cluster at the beginning.
	The distance between each cluster is computed, and the two nearest clusters are merged together.
	The above step is iterated till a single cluster is formed.
AgglomerativeClustering from sklearn.cluster can be used for achieving this.
Merging of two clusters can be any of the following linkage type: ward, complete or average.

3. Density Based Clustering: cluster core samples (dense area of a dataser) and denote non-core samples (sparse portion of dataset)
	Used to identify collective outliers 
	In Vision Processing (specially in self driving cars)
DBSCAN from sklearn.cluster is used for this purpose.
outlier should not exceed more than 5%
Important parameters: eps and min_samples

from sklearn.cluster import DBSCAM



4. Mean Shift Clustering: Mean Shift Clustering aims at discovering dense areas.

Steps Involved:
	Identify blob areas with randomly guessed centroids.
	Calculate the centroid of each blob area and shift to a new one, if there is a difference.
	Repeat the above step till the centroids converge.
make_blobs from sklearn.cluster can be used to initialize the blob areas. MeanShift from sklearn.
cluster can be used to perform Mean Shift clustering.

5. Affinity Propagation: generates clusters by passing messages between pairs of data points, until convergence.

AffinityPropagation class from sklearn.cluster can be used.
The above class can be controlled with two major parameters:
	preference: It controls the number of exemplars to be chosen by the algorithm.
	damping: It controls numerical oscillations while updating messages.

6. Spectral Clustering: is ideal to cluster data that is connected, and may not be in a compact space.

In general, the following steps are followed:
	Build an affinity matrix of data points.
	Embed data points in a lower dimensional space.
	Use a clustering method like k-means to partition the points on lower dimensional space.
spectral_clustering from sklearn.cluster can be used for achieving this.

# Demo of KMeans - An example of performing KMeans clustering is shown below

from sklearn.cluster import KMeans
kmeans_cluster = KMeans(n_clusters=2)
kmeans_cluster = kmeans_cluster.fit(X_train) 
kmeans_cluster.predict(X_test)

Evaluating a Clustering algorithm - A clustering algorithm is majorly evaluated using the following scores:
	i) Homogeneity: Evaluates if each cluster contains only members of a single class.
	ii) Completeness: All members of a given class are assigned to the same cluster.
	iii) V-measure: Harmonic mean of Homogeneity and Completeness.
	vi) Adjusted Rand index: Measures similarity of two assignments.

#Evaluation with scikit-learn
from sklearn import metrics
print(metrics.homogeneity_score(kmeans_cluster.predict(X_test), Y_test))
print(metrics.completeness_score(kmeans_cluster.predict(X_test), Y_test))
print(metrics.v_measure_score(kmeans_cluster.predict(X_test), Y_test))
print(metrics.adjusted_rand_score(kmeans_cluster.predict(X_test), Y_test))
